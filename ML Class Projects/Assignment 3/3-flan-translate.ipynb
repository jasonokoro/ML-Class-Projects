{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1001c433-4fca-4ba3-a670-10275b9274ad",
   "metadata": {},
   "source": [
    "# Part 3: Translation with Flan-T5\n",
    "\n",
    "In this part you will experiment with Google Flan-T5 models for machine translation. The original T5 model was a unifed sequence-to-sequence encoder-decoder architecture pretrained on a variety of tasks including machine translation. The Flan line of models improved on the performance of the original T5 series.\n",
    "\n",
    "In this part we will only apply the models, not train them. We will evaluate our results using the [Bleu score](https://en.wikipedia.org/wiki/BLEU), a common (but not perfect) quantitative metric for evaluating the quality of translations. Flan-T5 also comes in several different model sizes: We will study the impact of model size on performance by considering several different versions of Flan-T5.\n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Examine an encoder-decoder sequence-to-sequence Flan-T5 transformer model\n",
    "2. Apply Flan-T5 models to perform machine translation\n",
    "3. Evaluate the quality of machine translations by computing Bleu scores with respect to reference translations\n",
    "4. Study the affect\n",
    "\n",
    "While it is possible to complete this assignment using CPU compute, it may be slow. To accelerate your training, consider using GPU resources such as `CUDA` through the CS department cluster. Alternatives include Google colab or local GPU resources for those running on machines with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5249219-8c67-4087-b2be-3f6c9982d951",
   "metadata": {},
   "source": [
    "First, ensure that you have the `transformers` and `datasets` modules installed. We will use these modules for importing tokenizers, pretrained models, and datasets. You can run the following cells to try to install them with `pip` if needed. If you are using ondemand, ideally you would simply include `module load transformers` and `module load datasets` when making your initial reservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb3fa3-39f4-45ca-989d-31019c339100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f166e33-29ad-45b3-93f1-96b3f8173b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9aba23-9b2c-4aec-beda-a00cefa676df",
   "metadata": {},
   "source": [
    "First we import the `flan-t5` tokenizer (shared across all model sizes) and demonstrate its characteristics and usage. Note that the API is the same as we saw previously for the `BERT` model -- may want to review the extra details in that earlier part.\n",
    "\n",
    "Note that the example contains both English and French text.\n",
    "\n",
    "(If you have trouble downloading the tokenizer, it is possible that you need to install the `sentencepiece` module, for example by `pip install sentencepiece`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e637e38-7a1f-4eb3-91b4-e080daf23234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "print(\"Vocabulary size: \", tokenizer.vocab_size)\n",
    "tokenized = tokenizer([\"The little black cat sleeps in the window\", \n",
    "                       \"Le petit chat noir dort dans la fenÃªtre\"], padding='longest')\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b009142-7ad6-423d-9447-cf362ac3e6ae",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Below we import and preview the `flan-t5` model, beginning with the small version. You will also note that we are using 16-bit float representations to save on memory (this may be particularly relevant if you are using GPU compute for larger models, where the GPU may have limited memory available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef167a43-0dba-41eb-bb2f-7966b1a9b34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run, but you do not need to modify this code\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.float16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe34963-7122-4021-bb4b-63b1d210c480",
   "metadata": {},
   "source": [
    "Examine the `model.parameters()`. How much memory (in kilobytes (KB), megabytes (MB), or gigabytes (GB)) should it take to store the model itself, given the 16-bit (or 2 byte) precision specified in the import? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41035ae7-9718-4d2e-acab-7d6d26117c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write code for task 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b89f06-b05c-4882-b0fd-708fd2ed34a3",
   "metadata": {},
   "source": [
    "*Briefly explain for task 1 here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9a63d-7d7f-481f-a7a8-e7d87143c297",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Now we import and demonstrate the basic usage of the model `generate` method. This method autoregressively generates new text as we have discussed before in the context of causal language modeling, and supports different approaches (greedy, beam search, and sampling). The `generate` method API is [documented here](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/text_generation#transformers.GenerationMixin.generate).\n",
    "\n",
    "The example below demonstrates encoding a *batch* of inputs and passing them to the model for autoregressive generation. The printed output is generated by the model for the first and second input in the batch respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57177033-b02f-4e4a-afb0-f380313e3731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "input_text = [\"The little black cat sleeps in the window\", \"The dog runs in the field\"]\n",
    "encoded = tokenizer(input_text, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "outputs = model.generate(**encoded, max_new_tokens=100)\n",
    "print(outputs)\n",
    "\n",
    "for out in outputs:\n",
    "    print(tokenizer.decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa24589-2848-4928-b8c4-5869b9e31112",
   "metadata": {},
   "source": [
    "Note that the model did not translate the inputs. That is by design: Flan-T5 was pretrained on several different tasks including but not limited to machine translation.\n",
    "\n",
    "In order to use the model for translation, we need to **prompt it** to do so, providing context (literally) connecting to its pretraining. There are several different promptings that should work, we demonstrate two below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8c4dc-11cd-401f-bb8a-c08fe7baba8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "input_text = [\"The little black cat sleeps in the window\", \"The dog runs in the field\"]\n",
    "prompt = \"Translate English into French: \"\n",
    "prompted_text = [prompt + in_text for in_text in input_text]\n",
    "encoded = tokenizer(prompted_text, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "outputs = model.generate(**encoded, max_new_tokens=100)\n",
    "\n",
    "for out in outputs:\n",
    "    print(tokenizer.decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5577a4f-fbb6-43dc-a0cc-9164af8d2ab7",
   "metadata": {},
   "source": [
    "For this task, your goal is use the model to translate a large collection of German text into English, drawing from a paired translation of the novel Jane Eyre. Below we download and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2655d1-f62a-4e32-a083-c6dc45085ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OpusDataset(Dataset):\n",
    "    def __init__(self, dataset_stream, num_examples):\n",
    "        # Convert streaming dataset to list for random access\n",
    "        self.examples = list(dataset_stream.take(num_examples))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "dataset_stream = load_dataset(\n",
    "    \"opus_books\",\n",
    "    \"de-en\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Create instance of custom dataset\n",
    "dataset = OpusDataset(dataset_stream, num_examples=500)\n",
    "\n",
    "# Print a few examples to verify\n",
    "for i in range(100, 103):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"German: {dataset[i]['translation']['de']}\")\n",
    "    print(f\"English: {dataset[i]['translation']['en']}\")\n",
    "print(f\"\\nTotal examples loaded: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded5cd5-b650-4057-a19a-3265eb25f0b3",
   "metadata": {},
   "source": [
    "**Use the Flan-T5 model to translate all of the German text in the dataset into English.** Even with GPU compute, this may take several minutes, but should not take hours. We encourage you to add some output every 10 or 50 examples so that you can track the progress, though you are not required to do so.\n",
    "\n",
    "**Select at least three examples from the dataset and print the model translation as well as the real English text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5fc49-28eb-4ce5-8297-0999c70eed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for task 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3308a-f0ab-429a-bbe4-71b80bac168e",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Translations are difficult to evaluate quantitatively and without expert human translators. One common metric is the [BLEU score](https://en.wikipedia.org/wiki/BLEU).\n",
    "\n",
    "The below example demonstrates calculating BLEU scores with the `evaluate` module from Hugging Face. You can [see the documentation here](https://huggingface.co/spaces/evaluate-metric/bleu). The first value in the results dictionary gives the score. Normally the score is reported on a scale from 0-100; this implementation reports it on a 0-1 scale. Higher values are better, but scores of 1 are not necessarily expected given the many possible ways to translate.\n",
    "\n",
    "Note that `predictions` is a list of strings, but `references` is a list of lists of strings. This is because a single predicted translation could potentially have multiple equally good reference translations. In our case however we just have the single translation per pair, so `references` will be a list of lists, each with a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535dea9-e0f8-4b4d-831c-06fb4d32cca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "import evaluate\n",
    "\n",
    "predictions = [\"the black cat is sleeping in the sun by the window\", \n",
    "               \"the dog runs in the field while it rains\"]\n",
    "references = [[\"the black cat is sleeps on the sun by the window\"], \n",
    "              [\"the dog run in the field while it rain\"]]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c882937-f116-45bd-bb4c-657fde8d4262",
   "metadata": {},
   "source": [
    "**Calculate the `BLEU` score of your translations from task 2 against the real English text`. Report your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495ae5a-a30c-4849-b0e4-95334a440b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for task 3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c4927-f698-4bf0-a5f1-9863e99643cf",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4848987-6ee2-408d-a96d-670f0f546ba0",
   "metadata": {},
   "source": [
    "In this task, study the impact of model scale on the quality of the resulting translations. Earlier we used `model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.float16)` to import a `flan-t5-small` model. \n",
    "\n",
    "**Use two additional models: `flan-t5-base` and `flan-t5-large` to generate translations of the same dataset. Evaluate and report the BLEU score of both translations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116da63c-ea78-4bb3-a2ef-d22c0089b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for task 4 here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
