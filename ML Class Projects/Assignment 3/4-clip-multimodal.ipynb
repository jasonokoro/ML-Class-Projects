{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuu3dLhAON6C"
   },
   "source": [
    "# Part 4: CLIP Image-Vision Transformer\n",
    "\n",
    "In this part of the assignment, you'll dive into using the CLIP model. CLIP is a transformer model built by OpenAI designed to leverage the relationships of both images and text to improve understanding and interpretation. It is a *multimodal* model, in the sense that it takes both text and images as inputs. You can [read more about CLIP here.](https://openai.com/index/clip/).\n",
    "\n",
    "You won't need to implement the model structure from scratch. Instead, you'll leverage a pre-trained CLIP model for image classification. What is particularly interesting is that CLIP is not trained to predict any particular image labels: Rather, you can provide an arbitrary set of text labels and CLIP will try to predict a score for each text label given to consider as input. You will consider designing better text label prompts to improve performance, a process often referred to as [prompt engineering](https://www.promptingguide.ai/).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Through this exercise, you will:\n",
    "1. **Run** a pre-trained multimodal CLIP model on a sample image and examine the output.\n",
    "2. **Evaluate** the model's accuracy for classification.\n",
    "3. **Experiment** with prompt engineering techniques to optimize the model's classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is possible to complete this assignment using CPU compute, it may be slow. To accelerate your training, consider using GPU resources such as `CUDA` through the CS department cluster. Alternatives include Google colab or local GPU resources for those running on machines with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we download the CLIP model, its preprocessor (which takes care of tokenization for the text input and image preprocessing for the image input), and the Oxford pets dataset. The dataset consists of around 3,700 images of dogs or cats, with class labels broken down by the type/breed of dog or cat. Our goal will be to classify the breed of an image using CLIP.\n",
    "\n",
    "The labels in the dataset are the indices into `class_names`, the list of breed names. We also define a `class_to_species` map defined for every breed to track whether that breed is a dog or cat, information that might come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BHa13GPNDjo",
    "outputId": "7e08dd27-91d2-45ff-c577-976394f3c616",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run but you do not need to modify this code\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10, OxfordIIITPet\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "pets = OxfordIIITPet(root=os.path.expanduser(\"~/.cache\"), download=True, split=\"test\")\n",
    "class_names = pets.classes\n",
    "class_to_species = {\n",
    "    # Cats\n",
    "    'Abyssinian': 'cat', 'Bengal': 'cat', 'Birman': 'cat', 'Bombay': 'cat',\n",
    "    'British Shorthair': 'cat', 'Egyptian Mau': 'cat', 'Maine Coon': 'cat',\n",
    "    'Persian': 'cat', 'Ragdoll': 'cat', 'Russian Blue': 'cat', 'Siamese': 'cat',\n",
    "    'Sphynx': 'cat',\n",
    "    # Dogs\n",
    "    'American Bulldog': 'dog', 'American Pit Bull Terrier': 'dog', 'Basset Hound': 'dog',\n",
    "    'Beagle': 'dog', 'Boxer': 'dog', 'Chihuahua': 'dog', 'English Cocker Spaniel': 'dog', \n",
    "    'English Setter': 'dog', 'German Shorthaired': 'dog', 'Great Pyrenees': 'dog',\n",
    "    'Havanese': 'dog', 'Japanese Chin': 'dog', 'Keeshond': 'dog', 'Leonberger': 'dog',\n",
    "    'Miniature Pinscher': 'dog', 'Newfoundland': 'dog', 'Pomeranian': 'dog', 'Pug': 'dog',\n",
    "    'Saint Bernard': 'dog', 'Samoyed': 'dog', 'Scottish Terrier': 'dog', 'Shiba Inu': 'dog',\n",
    "    'Staffordshire Bull Terrier': 'dog', 'Wheaten Terrier': 'dog', 'Yorkshire Terrier': 'dog',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zax84uEcOuT1"
   },
   "source": [
    "Below we visualize the first image as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "VjY-scDoODop",
    "outputId": "e5ed2684-280f-4bf6-f808-958597c1423c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run, but you do not need to modify this code\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the image and ground truth label for first image as an example\n",
    "image, class_id = pets[0]\n",
    "print(class_id)\n",
    "print(class_names[class_id])\n",
    "      \n",
    "# Visualize the image\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we demonstrate the use of the CLIP model on the example image shown above. Note that we pass list of strings for the `text` parameter, simply corresponding to the string names of the breeds. The `predict_class` function expects inputs encoded with the `CLIP` preprocessor as shown. It returns the label (an index into the `class_names` list) with the maximum output logit for the model across all the strings in the input `text`. In this case, it correctly identifies the image as an `Abyssinian` by returning index `0` (`Abyssinian` is the first element of `class_names`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RZjxlX_OGPB",
    "outputId": "d28245e8-2984-4aae-fa7f-dbf81de8ff72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run, but you do not need to modify this code\n",
    "\n",
    "def predict_class(model, inputs):\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    return torch.argmax(logits_per_image, dim=1).item()\n",
    "\n",
    "\n",
    "example_inputs = preprocess(\n",
    "    text=[c for c in class_names],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(predict_class(model, example_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLoY0i08PYDw"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "In this part, your goal is to use CLIP to make predictions for the entire `pets` dataset and compute the overall accuracy of the predictions. You can use the relatively simple `text` prompting (just using the class names) as shown above. You should be able to get an accuracy of around 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# todo: write code for task 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUxvEY_wP3Oy"
   },
   "source": [
    "## Task 2\n",
    "\n",
    "Experiment with prompt engineering to improve the model's performance and achieve an accuracy of at least 87% on the dataset. By prompt engineering, we mean changing the `text` inputs to the model to provide more detail than just the class name. For examples:\n",
    "- You might include some instruction prior to the class name such as \"Picture of a: \", or \"Photo of a pet with breed: \"\n",
    "- You might also incorporate some additional information to help the model understand each particular class name. For example, you might use the class_to_species map from above to supplement with information like \", which is a type of cat\" when referring to a class name like Abyssinian\n",
    "\n",
    "Show your final results including the calculation of the accuracy of the predictions on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DjRA1CVcQJ1_"
   },
   "outputs": [],
   "source": [
    "# todo: write your code for task 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_clFiTmQtEu"
   },
   "source": [
    "## Task 3\n",
    "\n",
    "We could, of course, have simply trained a convolutional neural network to classify the pet images in this task. In one or two paragraphs, explain why the CLIP model is a more general model and explain one or more applications you could imagine developing using a model of this form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyavXKOpQyFX"
   },
   "source": [
    "*Briefly explain for task 3 here*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
